{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Spin Glass NADE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dinesh110598/Spinglass_NADE/blob/main/main.ipynb)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "import tensorflow.math as tfm\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from numba import jit, prange"
   ]
  },
  {
   "source": [
    "If you're running on your local machine, simply download the library.py and TrainData20k.npy files of the repo to the folder you're running this notebook on."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Colab Instructions\n",
    "Run the following cell if running on Google colaboratory notebook."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  7774  100  7774    0     0   4970      0  0:00:01  0:00:01 --:--:--  4970\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    14  100    14    0     0      7      0  0:00:02  0:00:01  0:00:01     7\n"
     ]
    }
   ],
   "source": [
    "!curl -o library.py https://raw.githubusercontent.com/dinesh110598/Spinglass_NADE/main/library.py\n",
    "!curl -o TrainingData20k.npy https://raw.githubusercontent.com/dinesh110598/Spinglass_NADE/main/TrainData20k.npy"
   ]
  },
  {
   "source": [
    "## Brief Introduction\n",
    "We aim to construct a neural network (Neural Autoregressive Distribution Estimator) that can efficiently approximate the Boltzmann distribution of an EA spin glass system in equilibrium via the training data obtained using annealed MCMC simulations. See here for more details: https://arxiv.org/abs/2002.04292\n",
    "\n",
    "The nitty gritty details of the neural network are coded in the library.py whose classes we'll be importing into this notebook directly:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from library import NADE_orig, NADE_fast"
   ]
  },
  {
   "source": [
    "We have performed MCMC simulations of the EA lattice at T=0.5 externally and loading the numpy data for the same into here. Overall, we have 20000 latices of 20x20 EA lattices with Gaussian couplings to train our network with. In order to reduce the memory burden on the neural network, we made sure that *all latices have spins at (0,0) position = 1* and noting that multiplying the entire lattice by -1 gives an energetically equivalent configuration that the neural network need not learn"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "t_lattice = np.load ('Traindata20k.npy')\n",
    "np.all (t_lattice[:,0,0] == 1) #Checks if (0,0) spins=1"
   ]
  },
  {
   "source": [
    "Let's calculate the energy of each lattice in the training data after loading coupling constants from \"NADEcouplings.npy\" file:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(20, 20, 2)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "Jnn = np.load (\"NADE_couplings.npy\")\n",
    "Jnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit (nopython=True, parallel=True)\n",
    "def calc_energy (Jnn, lattice):\n",
    "    def bvc (x):\n",
    "        if x == 20:\n",
    "            return 0\n",
    "        else:\n",
    "            return x\n",
    "    \n",
    "    energy = np.zeros (lattice.shape[0], np.float32)\n",
    "    for n in prange (lattice.shape[0]):\n",
    "        for i in prange (lattice.shape[1]):\n",
    "            for j in prange (lattice.shape[2]):\n",
    "                energy[n] -= Jnn[i,j,0]*lattice[n,i,j]*lattice[n,i+1,j]\n",
    "                energy[n] -= Jnn[i,j,1]*lattice[n,i,j]*lattice[n,i,j+1]\n",
    "    return energy/400"
   ]
  },
  {
   "source": [
    "Here, we convert the NumPy array to a tf.data datset so that we can batch and loop over it conveniently while training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices (t_lattice)\n",
    "train_data = train_data.batch(20)"
   ]
  },
  {
   "source": [
    "### NADE_orig model\n",
    "In the Reference 1, the neural network used there specified there evaluates the conditional probabilities of all N spins one by one. This network is built inside the NADE_orig class we've defined in *library*. Skip this section if it's too much waiting for training! Let's train and evaluate an object of this class: "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NADE_orig (inshape=(20,20),num_hidden=20)"
   ]
  },
  {
   "source": [
    "We'll call train_step method of the NADE_orig class on a sample of the training data. This step builds the \"autograph\" associated with the method to provide significant speedup to subsequent executions of the method"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TensorShape([])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "loss = model.train_step (tf.constant (t_lattice[0:20,:,:]))\n",
    "tf.print (loss)"
   ]
  },
  {
   "source": [
    "Though we can use fit() method available for keras models, we'll quickly write a custom training loop to keep things transparent and more flexible:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training loss (for one batch) at step 1: 0.6892\n",
      "20 samples seen so far\n",
      "Training loss (for one batch) at step 21: 0.6900\n",
      "420 samples seen so far\n",
      "Training loss (for one batch) at step 41: 0.6902\n",
      "820 samples seen so far\n",
      "Training loss (for one batch) at step 61: 0.6899\n",
      "1220 samples seen so far\n",
      "Training loss (for one batch) at step 81: 0.6898\n",
      "1620 samples seen so far\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "for epoch in range (epochs):\n",
    "    for step, data in enumerate(train_data):\n",
    "        loss = model.train_step (data)\n",
    "        if step%50 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step+1, float(loss))\n",
    "            )\n",
    "            print(\"%d samples seen so far\" % ((step + 1) * 20))"
   ]
  },
  {
   "source": [
    "We can now sample from the distribution learnt by the model so far:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.075"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "np.mean (x)"
   ]
  },
  {
   "source": [
    "### NADE_fast model\n",
    "The model originally prescribed in the paper is clearly too slow to train, particularly when we're looping over all N spins. This can however be speeded up by specifying a neural network that can evaluate all the N probabilities simultaneosly with the same architecture as originally prescribed in the paper. NADE_fast class here does the same:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_model = NADE_fast ((20,20), 20)"
   ]
  },
  {
   "source": [
    "We can call the untrained model on a part of our training data. It returns values of probabilities around 0.5, with the way we've initialised the model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20,), dtype=float32, numpy=\n",
       "array([0.49699467, 0.50090426, 0.50145364, 0.50024694, 0.4977393 ,\n",
       "       0.4973164 , 0.49811655, 0.49880856, 0.49776128, 0.49830493,\n",
       "       0.5017334 , 0.49851722, 0.4992549 , 0.5011461 , 0.5000467 ,\n",
       "       0.4996889 , 0.49932244, 0.49956924, 0.49838352, 0.49740195],\n",
       "      dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "fast_model.call (tf.constant(t_lattice[:20,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 9\nTraining loss (for one batch) at step 951: 0.3317\n19020 samples seen so far\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for epoch in range (epochs):\n",
    "    fast_model.loss_tracker.reset_states()\n",
    "    for step, data in enumerate(train_data):\n",
    "        loss = fast_model.train_step (data)\n",
    "        if step%50 == 0:\n",
    "            clear_output (wait=True)\n",
    "            print(\"Epoch: %d\" % (epoch,))\n",
    "            print(\"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step+1, float(loss)))\n",
    "            print(\"%d samples seen so far\" % ((step + 1) * 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_model.save_weights (\"FastNADEweights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20,), dtype=float32, numpy=\n",
       "array([0.8458787 , 0.8001719 , 0.72402114, 0.57857573, 0.8414492 ,\n",
       "       0.7879002 , 0.75884783, 0.8207925 , 0.8414858 , 0.77756923,\n",
       "       0.81860155, 0.50814086, 0.4804363 , 0.5107358 , 0.76328266,\n",
       "       0.65623343, 0.7918186 , 0.8549846 , 0.5463747 , 0.47044188],\n",
       "      dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "fast_model.call (t_lattice[60:80,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(20, 20, 20)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "lattice = np.stack([fast_model.sample() for _ in range(20)])\n",
    "lattice.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-1.1556046, -1.0963204, -1.065081 , -1.008458 , -1.0865479,\n",
       "       -0.9758622, -1.0738789, -1.0997452, -1.0888323, -1.1030087,\n",
       "       -1.0218664, -1.1054885, -1.0800701, -1.041604 , -1.0140287,\n",
       "       -1.0544673, -1.0273954, -1.0291573, -1.0895194, -1.0061578],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "calc_energy (Jnn, lattice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.02"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "x = np.arange ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}